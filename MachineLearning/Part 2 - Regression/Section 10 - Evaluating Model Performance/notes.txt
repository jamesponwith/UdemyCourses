---R-Squared---

SSres = SUM (Yi - Yi^)^2
SStot = SUM (Yi - Yavg)^2

R^2 = 1 - ( SSres / SStot)

The goal is to minimize SSres and SStot  because this represents the best fit line meaning that there 
is the minimum amount of distance between the data points and the line. 

While SSres and SStot become smaller R^2 is inching towards a value of 1 which is ideal but never happens.

---Adjusted R-Squared---

R^2 - Goodness of fit (greater the better)

Problem with this - when you add more variables to your model

if you add another variable will never decrease R^2

Adujusted R^2:

1 - (1 - R^2)(n-1 / n - p - 1)

p - number of regressors
n - sample size

Adjusted R squared penalizes the model for variables that do not help. 
(Penalized for each added varaible but balanced out if variable is helping model)

---

When looking at the statistics for a model (Like when we did with the backwards elimination process), 
even if the signifigance level falls slightly short of the 0.05 limit we imposed, we must also look 
at adjusted R squared to decide if our model is getting better or worse.

Remember: Adjusted R Squared indicates how strong your model is by being as close to 1 as possible


---

Interpreting Linear Regression Coefficients 

Positive - Correlated with dependent variable 
Negitive - Inversely correleated with dependent variable

Alwyas say magnatude in terms of units of the dependent variable

The per unit trick -- express the signifigance of the variables in terms of
per unit. For example, the multilinear regression model says that R&D has a
coefficient of (7.99 - 01) which can be interpretted to for every dollar extra
spent on R&D, an additional .79 cents will be added to profit.


